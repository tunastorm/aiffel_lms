{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expired-mambo",
   "metadata": {},
   "source": [
    "## 가위바위보_구별기\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-stocks",
   "metadata": {},
   "source": [
    "### 반복 사용되는 모듈 임포트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "medium-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, glob, cv2\n",
    "\n",
    "\n",
    "print(\"PIL 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-armstrong",
   "metadata": {},
   "source": [
    "### 가위, 바위, 보 이미지 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "contemporary-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test와 train 모두 처리하는 함수\n",
    "def resize_images(img_path): \n",
    "    for folder in os.listdir(img_path):\n",
    "        if folder not in ['rock', 'scissor', 'paper']: \n",
    "            continue   \n",
    "        images=glob.glob(img_path + f\"/{folder}/*.jpg\")  \n",
    "        print(f\"{len(images)} {folder} images to be resized.\")\n",
    "        # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "        target_size=(28,28)\n",
    "        for img in images:\n",
    "            old_img=cv2.imread(img,cv2.IMREAD_GRAYSCALE) # 사진을 흑백으로 변환한다.\n",
    "            new_img=cv2.resize(old_img,target_size,Image.ANTIALIAS)\n",
    "            cv2.imwrite(img,new_img)\n",
    "        print(f\"{len(images)} {folder} images resized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "constant-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/train 중 하나를 입력하세요test\n",
      "92 paper images to be resized.\n",
      "92 paper images resized.\n",
      "77 rock images to be resized.\n",
      "77 rock images resized.\n",
      "66 scissor images to be resized.\n",
      "66 scissor images resized.\n",
      "test 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "folder_name = input(\"test/train 중 하나를 입력하세요\")\n",
    "image_dir_path = os.getenv(\"HOME\") + f\"/aiffel/rock_scissor_paper/{folder_name}\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(f\"{folder_name} 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-formation",
   "metadata": {},
   "source": [
    "### 전처리된 이미지 행렬화 및 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "uniform-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test와 train 모두 처리하는 함수\n",
    "def load_data(img_path, number_of_data=300): \n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=1\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size, color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "     \n",
    "    # number_of_data를 3분의 1로 나눠 가위/바위/보를 균등하게 채움   \n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        if idx > (number_of_data//3)-1: # 가위 이미지의 개수상한 초과시 추가하지 않음.\n",
    "            continue\n",
    "        img = np.array(Image.open(file),dtype=np.uint8).reshape(28,28,1)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "        \n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        if idx > (number_of_data//3*2)-1: # 바위 이미지의 개수상한 초과시 추가하지 않음.\n",
    "            continue\n",
    "        img = np.array(Image.open(file),dtype=np.uint8).reshape(28,28,1)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "\n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        if idx > number_of_data-1:  # 보 이미지의 개수상한 초과시 추가하지 않음.\n",
    "            continue\n",
    "        img = np.array(Image.open(file),dtype=np.uint8).reshape(28,28,1)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(f\"처리된 데이터의 이미지 개수는 {idx}개 입니다.\")\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ranking-ending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test 중 하나를 입력하세요train\n",
      "처리된 데이터의 이미지 개수는 3207개 입니다.\n",
      "x_train shape: (3300, 28, 28, 1)\n",
      "y_train shape: (3300,)\n"
     ]
    }
   ],
   "source": [
    "folder_name = input(\"train/test 중 하나를 입력하세요\")\n",
    "image_dir_path = os.getenv(\"HOME\") + f\"/aiffel/rock_scissor_paper/{folder_name}\"\n",
    "if folder_name == \"train\":\n",
    "    (x_train, y_train)=load_data(image_dir_path,3300) # 최대 3300개의 학습데이터를 볼러올 수 잇다. \n",
    "    x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "    x_train_reshaped=x_train_norm.reshape( -1, 28, 28, 1)  # 정규화된 이미지 reshape\n",
    "    print(f\"x_train shape: {x_train_reshaped.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "else: \n",
    "    (x_test, y_test)=load_data(image_dir_path) # 기본값인 300개의 학습데이터를 볼러올 수 잇다. \n",
    "    x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "    x_train_reshaped=x_train_norm.reshape( -1, 28, 28, 1)  # 정규화된 이미지 reshape\n",
    "    x_test_reshaped=x_test_norm.reshape( -1, 28, 28, 1)\n",
    "    print(f\"x_test reshaped: {x_test_reshaped.shape}\")\n",
    "    print(f\"y_test reshaped: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-sixth",
   "metadata": {},
   "source": [
    "* train 데이터는 AIFFEL 동료들의 가위바위보이미지 3207장을 사용하였다. \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-tamil",
   "metadata": {},
   "source": [
    "### 학습 모델만들기  \n",
    "\n",
    "[컨볼루션 신경망 레이어 이야기](https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/)를 참조하여 실습을 진행했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ethical-blink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_50 (Conv2D)           (None, 25, 25, 16)        272       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 9, 9, 16)          4112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 21,872\n",
      "Trainable params: 21,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Sequential 모델 불러오기\n",
    "model = Sequential()\n",
    "#  28X28X1의 입력이미지를 입력받는 4X4 크기의 필터 16개가 탑재된 Conv2D레이어 추가  \n",
    "model.add(Conv2D(16, (4,4), activation='relu', input_shape=(28,28,1)))\n",
    "# Conv2D레이어의 출력이미지를 입력받아 2X2사이즈 이미지로 변환해 출력하는 MaxPool2D 레이어 추가\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(16, (4,4), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D((2,2))) # MaxPool2D = MaxPooling2D\n",
    "# 이미지를 1차원으로 바꾸어 주는 Flatton 레이어 추가\n",
    "model.add(Flatten())\n",
    "# 입력뉴런 4개 출력뉴런 64개인 Dense레이어 추가           \n",
    "model.add(Dense(64, activation='relu', input_dim=4))\n",
    "# 출력뉴런 16개인 Dense레이어를 model에 추가   \n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-vampire",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "subtle-marble",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "104/104 [==============================] - 1s 3ms/step - loss: 1.6504 - accuracy: 0.3328\n",
      "Epoch 2/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 1.0567 - accuracy: 0.4137\n",
      "Epoch 3/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.9989 - accuracy: 0.4548\n",
      "Epoch 4/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.9050 - accuracy: 0.5711\n",
      "Epoch 5/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.8176 - accuracy: 0.6124\n",
      "Epoch 6/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.7784 - accuracy: 0.6291\n",
      "Epoch 7/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.6987 - accuracy: 0.6941\n",
      "Epoch 8/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.6238 - accuracy: 0.7409\n",
      "Epoch 9/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.5564 - accuracy: 0.7572\n",
      "Epoch 10/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.5054 - accuracy: 0.7982\n",
      "Epoch 11/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.8297\n",
      "Epoch 12/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.8277\n",
      "Epoch 13/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.4127 - accuracy: 0.8475\n",
      "Epoch 14/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8678\n",
      "Epoch 15/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.3027 - accuracy: 0.8967\n",
      "Epoch 16/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.2907 - accuracy: 0.8890\n",
      "Epoch 17/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.2700 - accuracy: 0.9004\n",
      "Epoch 18/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.2354 - accuracy: 0.9221\n",
      "Epoch 19/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.1887 - accuracy: 0.9457\n",
      "Epoch 20/20\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.9473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82542879d0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델이 20번 학습한다.                                      \n",
    "model.fit(x_train_reshaped, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-newark",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "stretch-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 1.3219 - accuracy: 0.6967\n",
      "test_loss: 1.321900725364685\n",
      "test_accuracy: 0.6966666579246521\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "print(f'test_loss: {test_loss}')\n",
    "print(f'test_accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-reset",
   "metadata": {},
   "source": [
    "## 후기\n",
    "-----\n",
    "이번 과제를 수행하는 동안 어려웠던 점은 역시 **accuracy 60을 넘기는 것**이었다.\n",
    "\n",
    "오버피팅을 극복하기 위해 학습데이터의 크기를 처음 300장에서 3207장까지 늘리고 이미지를 그레이스케일로 변환하고 여러 시도를 했었다.\n",
    "\n",
    "그러다보니 99, 80, 67점 이렇게 3번이나 60을 넘겨보았지만, \n",
    "\n",
    "참지못하고 다시 시도하는 바람에 마지막 69점을 만들기까지 긴 시간이 걸렸다.\n",
    "\n",
    "하지만 그러면서 여러가지 시도를 하다보니 배우게 된 부분 역시 많았다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1. cv2 모듈을 활용한 이미지파일 그레이스케일 처리\n",
    "\n",
    "grayscale적용 자체는 검색으로 어렵지 않게 했지만, plt로 출력할 때 색상이 예상밖의 색으로 나와 당황스러웠다.\n",
    "\n",
    "lms조원들에게 물어보니 plt.imshow의 인자로 cmap='gray'를 넘겨줌으로써 출력시에도 흑백으로 설정해주면 되는 문제였다.\n",
    "\n",
    "모델 학습에는 지장이 없는 문제였으나 한동안 풀지 못했던 기억이 난다.\n",
    "\n",
    "\n",
    "#### 2. [keras의 sequential 모델과 그 레이어들의 사용법](https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/)\n",
    "\n",
    "위 링크의 글을 많이 참조하였는데, 학습모델생성시 사용되는 sequential 모델과 그 아래의 Conv2D, maxPooling2D, Faltton, Dense레이어들에 대한 사용법과 sequential모델의 전체 그림까지 설명이 잘되어 있어서 좋았다. 이 글을 참조하며 레이어들의 파라미터를 조절하다보니 69%까지 다시 도달할 수 있었다. 하지만 아직 컨볼루션 신경망 레이어의 구조가 완전히 잘 이해되지 않아 아쉬웠다.\n",
    "\n",
    "#### 3. 웹캠 영상 프레임 단위로 이미지 저장 코드 활용\n",
    "test 데이터를 생성할 때 lms조원이 구글링 하여 공유해준 웹캠 영상을 프레임 단위로 이미지 파일로 자동저장하는 아래의 스크립트를 활용해보았다. \n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "# open webcam (웹캠 열기)\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "sample_num = 0\n",
    "captured_num = 0\n",
    "\n",
    "# loop through frames\n",
    "while webcam.isOpened():\n",
    "\n",
    "    # read frame from webcam\n",
    "    status, frame = webcam.read()\n",
    "    sample_num = sample_num + 1\n",
    "\n",
    "    if not status:\n",
    "        break\n",
    "\n",
    "    # display output\n",
    "    cv2.imshow(\"captured frames\", frame)\n",
    "\n",
    "    if sample_num == 4:\n",
    "        captured_num = captured_num + 1\n",
    "        cv2.imwrite('C:/Users/PC/Pictures/RPS_images/rock/img' + str(captured_num) + '.jpg', frame)  # 바위 이미지 수집시\n",
    "        # cv2.imwrite('C:/Users/PC/Pictures/RPS_images/paper/img'+str(captured_num)+'.jpg', frame) # 보 이미지 수집시\n",
    "        # cv2.imwrite('C:/Users/PC/Pictures/RPS_images/scissors/img'+str(captured_num)+'.jpg', frame) # 가위 이미지 수집시\n",
    "        sample_num = 0\n",
    "\n",
    "    # press \"Q\" to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release resources\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "출처: [[Anaconda+python] 웹캠 영상 프레임 샘플링해서 저장하기(쉽게 이미지 데이터베이스 만들기).txt](https://bskyvision.com/723)\n",
    "\n",
    "코드자체는 가위/바위/보의 경로가 바뀌는 것을 수동으로 주석을 해제/설정 하는 것으로 스위칭하는 것 외엔 큰 문제가 없었으나, gather에서 cam을 사용할 경우 cam을 사용하지 못해 에러가 발생하는 문제가 있어서 잠시 캠을 끄고 이미지를 만들었던 기억이 난다. 급하게 이미지를 대량으로 만들어야할 필요가 있을 때 쓰면 가성비가 좋을 것 같다. \n",
    "\n",
    "\n",
    "#### 마무리\n",
    "이번 노드를 진행하면서 처음 머신러닝을 경험해볼 수 있어서 재미있게 노드를 진행했었다. 하지만 그와 동시에 내가 뭘 모르고 있는지 다시 느낄 수 있었다. 특히 구글링으로 찾아낸 블로그 글에서 기술한 컨볼루션 신경망 레이어의 구조를 보면서 내가 앞으로 배워야할 것이 이런 것이구나 하고 처음 제대로 알게된 기분이들었다. 지금은 낯설기만 한 것이지만 6개월이 지난 후의 나에게는 익숙한 것이 되어있었으면 좋겠고, 꼭 그렇게 만들고 싶다.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
